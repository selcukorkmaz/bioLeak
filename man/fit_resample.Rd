% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fit_resample.R
\name{fit_resample}
\alias{fit_resample}
\title{Fit and evaluate with leakage guards over predefined splits}
\usage{
fit_resample(
  x,
  outcome,
  splits,
  preprocess = list(impute = list(method = "median"), normalize = list(method =
    "zscore"), filter = list(var_thresh = 0, iqr_thresh = 0), fs = list(method = "none")),
  learner = c("glmnet", "ranger"),
  learner_args = list(),
  custom_learners = list(),
  metrics = c("auc", "pr_auc", "accuracy"),
  class_weights = NULL,
  positive_class = NULL,
  parallel = FALSE,
  refit = TRUE,
  seed = 1
)
}
\arguments{
\item{x}{SummarizedExperiment or matrix/data.frame}

\item{outcome}{outcome column (if x is SE)}

\item{splits}{LeakSplits object from make_splits()}

\item{preprocess}{list(impute, normalize, filter=list(...), fs)}

\item{learner}{parsnip model_spec (or list of model_spec objects) describing
the model(s) to fit. For legacy use, a character vector of learner names
(e.g., "glmnet", "ranger") or custom learner IDs is still supported.}

\item{learner_args}{list of additional arguments passed to legacy learners
(ignored when `learner` is a parsnip model_spec).}

\item{custom_learners}{named list of custom learner definitions used only
with legacy character learners. Each entry
must contain \code{fit} and \code{predict} functions. The \code{fit} function
should accept \code{x}, \code{y}, \code{task}, and \code{weights}, and return
a model object. The \code{predict} function should accept \code{object},
\code{newdata}, and \code{task}, and return numeric predictions.}

\item{metrics}{named list of metric functions or vector of metric names}

\item{class_weights}{optional named numeric vector of weights for binomial outcomes}

\item{positive_class}{optional value indicating the positive class for binomial outcomes.
When set, the outcome levels are reordered so that \code{positive_class} is treated
as the positive class (level 2). If NULL, the second factor level is used.}

\item{parallel}{logical, use future.apply for multicore execution}

\item{refit}{logical, if TRUE retrain final model on full data}

\item{seed}{integer, for reproducibility}
}
\value{
LeakFit object
}
\description{
Performs cross-validated model training and evaluation using
leakage-protected preprocessing (.guard_fit) and user-specified learners.
}
\details{
Preprocessing is fit on the training fold and applied to the test fold,
preventing leakage from global imputation, scaling, or feature selection.
Use \code{learner_args} to pass model-specific arguments, either as a named
list keyed by learner or a single list applied to all learners. For custom
learners, \code{learner_args[[name]]} may be a list with \code{fit} and
\code{predict} sublists to pass distinct arguments to each stage. For binomial
tasks, predictions and metrics assume the positive class is the second factor
level; use \code{positive_class} to control this. Parsnip learners must support
probability predictions for binomial metrics (AUC/PR-AUC/accuracy).
}
\examples{
\dontrun{
set.seed(1)
df <- data.frame(
  subject = rep(1:10, each = 2),
  outcome = rbinom(20, 1, 0.5),
  x1 = rnorm(20),
  x2 = rnorm(20)
)
splits <- make_splits(df, outcome = "outcome",
                      mode = "subject_grouped", group = "subject", v = 5)
fit <- fit_resample(df, outcome = "outcome", splits = splits,
                    learner = "glmnet", metrics = "auc")
summary(fit)

# Parsnip model_spec
\dontrun{
if (requireNamespace("parsnip", quietly = TRUE)) {
  spec <- parsnip::boost_tree(mode = "classification", trees = 200) |>
    parsnip::set_engine("xgboost")
  fit3 <- fit_resample(df, outcome = "outcome", splits = splits,
                       learner = spec, metrics = "auc")
}
}

# Custom learner (logistic regression)
custom <- list(
  glm = list(
    fit = function(x, y, task, weights, ...) {
      stats::glm(y ~ ., data = as.data.frame(x),
                 family = stats::binomial(), weights = weights)
    },
    predict = function(object, newdata, task, ...) {
      as.numeric(stats::predict(object, newdata = as.data.frame(newdata), type = "response"))
    }
  )
)
fit2 <- fit_resample(df, outcome = "outcome", splits = splits,
                     learner = "glm", custom_learners = custom,
                     metrics = "accuracy")
}
}
